{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import gymnasium as gym\n",
    "import stable_baselines3 as stb\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIMITRIS\\AppData\\Local\\Temp\\ipykernel_22320\\4119366825.py:73: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  state_batch = torch.FloatTensor([exp[0] for exp in batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -280.8753187456331\n",
      "Episode 2, Total Reward: -213.04787505672826\n",
      "Episode 3, Total Reward: -185.73228888754053\n",
      "Episode 4, Total Reward: -95.74447776198635\n",
      "Episode 5, Total Reward: -200.75514298349992\n",
      "Episode 6, Total Reward: -160.01263329974586\n",
      "Episode 7, Total Reward: -137.9937803653483\n",
      "Episode 8, Total Reward: -114.28593634645749\n",
      "Episode 9, Total Reward: -163.99764781665442\n",
      "Episode 10, Total Reward: -89.43146424364397\n",
      "Episode 11, Total Reward: -120.72758590662622\n",
      "Episode 12, Total Reward: -207.3516905235723\n",
      "Episode 13, Total Reward: -301.62450856618807\n",
      "Episode 14, Total Reward: -90.41218028976866\n",
      "Episode 15, Total Reward: -110.27022758258401\n",
      "Episode 16, Total Reward: -121.57356595850742\n",
      "Episode 17, Total Reward: -20.71741146816575\n",
      "Episode 18, Total Reward: -302.60374614490036\n",
      "Episode 19, Total Reward: -59.68022407489589\n",
      "Episode 20, Total Reward: -128.1451201042813\n",
      "Episode 21, Total Reward: -121.98241617557072\n",
      "Episode 22, Total Reward: -227.6087757175111\n",
      "Episode 23, Total Reward: -277.5554799266448\n",
      "Episode 24, Total Reward: -280.8106670967094\n",
      "Episode 25, Total Reward: -107.59817966500034\n",
      "Episode 26, Total Reward: -193.6835232100536\n",
      "Episode 27, Total Reward: -26.14417706510916\n",
      "Episode 28, Total Reward: -95.36962320648189\n",
      "Episode 29, Total Reward: -129.15882708463528\n",
      "Episode 30, Total Reward: -73.55074766806148\n",
      "Episode 31, Total Reward: -92.49850896210924\n",
      "Episode 32, Total Reward: -240.1566057268381\n",
      "Episode 33, Total Reward: -130.6540163465193\n",
      "Episode 34, Total Reward: -129.3870090505203\n",
      "Episode 35, Total Reward: -39.190409996194134\n",
      "Episode 36, Total Reward: -219.14525241273242\n",
      "Episode 37, Total Reward: -212.98114821506084\n",
      "Episode 38, Total Reward: -163.05957713246966\n",
      "Episode 39, Total Reward: -213.44857964965485\n",
      "Episode 40, Total Reward: -211.10126548493952\n",
      "Episode 41, Total Reward: -62.585879286953045\n",
      "Episode 42, Total Reward: -136.42705889785987\n",
      "Episode 43, Total Reward: -97.91814016021215\n",
      "Episode 44, Total Reward: -87.1164824662651\n",
      "Episode 45, Total Reward: -231.29051397314663\n",
      "Episode 46, Total Reward: -80.58804744851409\n",
      "Episode 47, Total Reward: -2.857326284730405\n",
      "Episode 48, Total Reward: -101.07219926000464\n",
      "Episode 49, Total Reward: -85.32175813609173\n",
      "Episode 50, Total Reward: -58.618773052172095\n",
      "Episode 51, Total Reward: -102.79250821654898\n",
      "Episode 52, Total Reward: -55.102435875821755\n",
      "Episode 53, Total Reward: -61.85822948530826\n",
      "Episode 54, Total Reward: -123.93801301076928\n",
      "Episode 55, Total Reward: -74.32784097927058\n",
      "Episode 56, Total Reward: -109.34995799258826\n",
      "Episode 57, Total Reward: -7.923082204121002\n",
      "Episode 58, Total Reward: -103.92268924191251\n",
      "Episode 59, Total Reward: -140.459994650903\n",
      "Episode 60, Total Reward: -110.80867729105199\n",
      "Episode 61, Total Reward: -111.7119094509213\n",
      "Episode 62, Total Reward: -154.00382723911054\n",
      "Episode 63, Total Reward: -225.04587027074967\n",
      "Episode 64, Total Reward: -77.01260385631095\n",
      "Episode 65, Total Reward: -63.54511111209849\n",
      "Episode 66, Total Reward: -113.70133864823566\n",
      "Episode 67, Total Reward: -134.6595995375344\n",
      "Episode 68, Total Reward: -290.5105812173704\n",
      "Episode 69, Total Reward: -69.61304254192939\n",
      "Episode 70, Total Reward: -173.06398563488142\n",
      "Episode 71, Total Reward: -20.00156337372016\n",
      "Episode 72, Total Reward: 44.83093017564738\n",
      "Episode 73, Total Reward: -68.60023307990053\n",
      "Episode 74, Total Reward: -167.95727509298075\n",
      "Episode 75, Total Reward: -59.50857379174073\n",
      "Episode 76, Total Reward: -119.37915480778096\n",
      "Episode 77, Total Reward: -211.07848034780923\n",
      "Episode 78, Total Reward: -101.13118241731799\n",
      "Episode 79, Total Reward: -14.185173390955413\n",
      "Episode 80, Total Reward: -13.301181702204886\n",
      "Episode 81, Total Reward: -64.00265004064043\n",
      "Episode 82, Total Reward: -67.94814768421948\n",
      "Episode 83, Total Reward: -89.37104947470127\n",
      "Episode 84, Total Reward: -313.7161479787144\n",
      "Episode 85, Total Reward: 22.996821728384262\n",
      "Episode 86, Total Reward: -189.8536648112533\n",
      "Episode 87, Total Reward: 11.7337355439589\n",
      "Episode 88, Total Reward: -80.89605858930999\n",
      "Episode 89, Total Reward: -67.74699417630973\n",
      "Episode 90, Total Reward: -70.1440004299001\n",
      "Episode 91, Total Reward: -102.3928354844685\n",
      "Episode 92, Total Reward: -256.1471352236957\n",
      "Episode 93, Total Reward: -40.521449492171556\n",
      "Episode 94, Total Reward: -98.60920407594625\n",
      "Episode 95, Total Reward: -45.89917141423432\n",
      "Episode 96, Total Reward: -93.90430450639627\n",
      "Episode 97, Total Reward: -82.91249017265807\n",
      "Episode 98, Total Reward: -54.13857169151517\n",
      "Episode 99, Total Reward: -43.16818296376097\n",
      "Episode 100, Total Reward: -78.78256727951549\n",
      "Episode 101, Total Reward: 3.1391174083869657\n",
      "Episode 102, Total Reward: -76.80646360494977\n",
      "Episode 103, Total Reward: -98.28302207652871\n",
      "Episode 104, Total Reward: -83.27579251474164\n",
      "Episode 105, Total Reward: -236.0126835586392\n",
      "Episode 106, Total Reward: -66.82648732754215\n",
      "Episode 107, Total Reward: -78.30374295530875\n",
      "Episode 108, Total Reward: -100.79127167318757\n",
      "Episode 109, Total Reward: -21.754582287352065\n",
      "Episode 110, Total Reward: -40.993779627570646\n",
      "Episode 111, Total Reward: -63.093305898537345\n",
      "Episode 112, Total Reward: -60.95348683851023\n",
      "Episode 113, Total Reward: -290.24070503086034\n",
      "Episode 114, Total Reward: 30.40421243346617\n",
      "Episode 115, Total Reward: -59.74142177970812\n",
      "Episode 116, Total Reward: -105.95765360035018\n",
      "Episode 117, Total Reward: 6.170724968805146\n",
      "Episode 118, Total Reward: -35.380978474134594\n",
      "Episode 119, Total Reward: -135.92241844675277\n",
      "Episode 120, Total Reward: 26.9754114386633\n",
      "Episode 121, Total Reward: 10.89811351231478\n",
      "Episode 122, Total Reward: 30.71607199799402\n",
      "Episode 123, Total Reward: -76.48826644107638\n",
      "Episode 124, Total Reward: -206.16759059691685\n",
      "Episode 125, Total Reward: -148.66289404719046\n",
      "Episode 126, Total Reward: -70.7110699517714\n",
      "Episode 127, Total Reward: -32.7334138504896\n",
      "Episode 128, Total Reward: -36.74658102916507\n",
      "Episode 129, Total Reward: -36.17315287097551\n",
      "Episode 130, Total Reward: -44.966367784082124\n",
      "Episode 131, Total Reward: -50.73230341811407\n",
      "Episode 132, Total Reward: -71.78762633803106\n",
      "Episode 133, Total Reward: -258.42993859687533\n",
      "Episode 134, Total Reward: -140.14390810025435\n",
      "Episode 135, Total Reward: -59.92160575123742\n",
      "Episode 136, Total Reward: -66.03222393218464\n",
      "Episode 137, Total Reward: -74.92410565909981\n",
      "Episode 138, Total Reward: -9.741405222060664\n",
      "Episode 139, Total Reward: -25.662965095433876\n",
      "Episode 140, Total Reward: -55.780805823611075\n",
      "Episode 141, Total Reward: -32.537976549838575\n",
      "Episode 142, Total Reward: -49.52154160124969\n",
      "Episode 143, Total Reward: 15.881223270721392\n",
      "Episode 144, Total Reward: -282.14847617194425\n",
      "Episode 145, Total Reward: -51.91567907768502\n",
      "Episode 146, Total Reward: -45.47031124129008\n",
      "Episode 147, Total Reward: -65.7792139752012\n",
      "Episode 148, Total Reward: -198.42704400917353\n",
      "Episode 149, Total Reward: -275.40201271468123\n",
      "Episode 150, Total Reward: -157.60285336413034\n",
      "Episode 151, Total Reward: -32.82668338033696\n",
      "Episode 152, Total Reward: -58.379918847444856\n",
      "Episode 153, Total Reward: -77.29140920912661\n",
      "Episode 154, Total Reward: 7.285181828859166\n",
      "Episode 155, Total Reward: 19.671624551967497\n",
      "Episode 156, Total Reward: -39.07685038587923\n",
      "Episode 157, Total Reward: -2.126549270073312\n",
      "Episode 158, Total Reward: -50.23354355246343\n",
      "Episode 159, Total Reward: -40.93454764213622\n",
      "Episode 160, Total Reward: -273.83493306496354\n",
      "Episode 161, Total Reward: 25.53506577922282\n",
      "Episode 162, Total Reward: 5.133415836561568\n",
      "Episode 163, Total Reward: 70.61317480890442\n",
      "Episode 164, Total Reward: -1.7174371155031167\n",
      "Episode 165, Total Reward: -17.008910069430897\n",
      "Episode 166, Total Reward: -233.53803797891612\n",
      "Episode 167, Total Reward: -79.48366490968853\n",
      "Episode 168, Total Reward: 18.589207415553915\n",
      "Episode 169, Total Reward: -13.102996914457819\n",
      "Episode 170, Total Reward: -58.01802336879028\n",
      "Episode 171, Total Reward: 50.68514072043109\n",
      "Episode 172, Total Reward: -43.90846240410378\n",
      "Episode 173, Total Reward: -273.17836754427697\n",
      "Episode 174, Total Reward: -166.95226480658556\n",
      "Episode 175, Total Reward: -28.82023709018327\n",
      "Episode 176, Total Reward: 5.000668757495561\n",
      "Episode 177, Total Reward: -19.72664925743854\n",
      "Episode 178, Total Reward: -128.66953858179411\n",
      "Episode 179, Total Reward: -127.15908414503804\n",
      "Episode 180, Total Reward: -96.1466262677609\n",
      "Episode 181, Total Reward: -234.9962088318969\n",
      "Episode 182, Total Reward: -26.669126320949815\n",
      "Episode 183, Total Reward: -120.7121982171657\n",
      "Episode 184, Total Reward: -205.29540165055016\n",
      "Episode 185, Total Reward: -170.42615452488485\n",
      "Episode 186, Total Reward: -208.5034490013092\n",
      "Episode 187, Total Reward: -96.03441151038894\n",
      "Episode 188, Total Reward: -20.18703505509488\n",
      "Episode 189, Total Reward: -7.653757768310387\n",
      "Episode 190, Total Reward: -29.12828051855934\n",
      "Episode 191, Total Reward: 9.613479740617123\n",
      "Episode 192, Total Reward: -89.62635674966002\n",
      "Episode 193, Total Reward: -318.62192618147867\n",
      "Episode 194, Total Reward: -18.368920479111893\n",
      "Episode 195, Total Reward: -219.8927600862009\n",
      "Episode 196, Total Reward: -174.09024698813863\n",
      "Episode 197, Total Reward: -225.29499696613658\n",
      "Episode 198, Total Reward: -230.3314934297091\n",
      "Episode 199, Total Reward: -146.35276524106393\n",
      "Episode 200, Total Reward: -45.08697685649353\n",
      "Episode 201, Total Reward: -231.46022539035437\n",
      "Episode 202, Total Reward: -241.5383226009245\n",
      "Episode 203, Total Reward: -44.262606863022086\n",
      "Episode 204, Total Reward: -143.20989348214275\n",
      "Episode 205, Total Reward: -103.24277038998234\n",
      "Episode 206, Total Reward: -304.3166210535402\n",
      "Episode 207, Total Reward: -197.89135504395185\n",
      "Episode 208, Total Reward: -55.54950725901353\n",
      "Episode 209, Total Reward: 41.89244185370407\n",
      "Episode 210, Total Reward: -237.11393903657108\n",
      "Episode 211, Total Reward: -80.7395188999972\n",
      "Episode 212, Total Reward: -165.4768373481785\n",
      "Episode 213, Total Reward: -29.725419342035373\n",
      "Episode 214, Total Reward: -3.6647920209206823\n",
      "Episode 215, Total Reward: -28.235133879776427\n",
      "Episode 216, Total Reward: -47.66620420594273\n",
      "Episode 217, Total Reward: -266.37745778762326\n",
      "Episode 218, Total Reward: -73.85057603793558\n",
      "Episode 219, Total Reward: -369.9014519269218\n",
      "Episode 220, Total Reward: -103.2891197326375\n",
      "Episode 221, Total Reward: -123.90912523428223\n",
      "Episode 222, Total Reward: 3.759429074693667\n",
      "Episode 223, Total Reward: -244.3603375311472\n",
      "Episode 224, Total Reward: -123.75460610923011\n",
      "Episode 225, Total Reward: -146.59619982959546\n",
      "Episode 226, Total Reward: -11.221831268451876\n",
      "Episode 227, Total Reward: -62.65389358374003\n",
      "Episode 228, Total Reward: -43.532079763701056\n",
      "Episode 229, Total Reward: -36.310137658975854\n",
      "Episode 230, Total Reward: -36.749107990153085\n",
      "Episode 231, Total Reward: -165.65104207428584\n",
      "Episode 232, Total Reward: -36.09929058144901\n",
      "Episode 233, Total Reward: -120.10711240269802\n",
      "Episode 234, Total Reward: -7.305852082147287\n",
      "Episode 235, Total Reward: 8.650055975809378\n",
      "Episode 236, Total Reward: -95.83436834575487\n",
      "Episode 237, Total Reward: -3.0952502021494395\n",
      "Episode 238, Total Reward: -139.54601541043263\n",
      "Episode 239, Total Reward: -206.66302959668207\n",
      "Episode 240, Total Reward: -20.35383511272355\n",
      "Episode 241, Total Reward: -81.36586207235756\n",
      "Episode 242, Total Reward: -57.81090749216436\n",
      "Episode 243, Total Reward: -12.851497660407196\n",
      "Episode 244, Total Reward: -21.993977332694215\n",
      "Episode 245, Total Reward: -10.729220174551433\n",
      "Episode 246, Total Reward: -101.19438118216223\n",
      "Episode 247, Total Reward: -58.731396457546225\n",
      "Episode 248, Total Reward: -55.70496062759661\n",
      "Episode 249, Total Reward: -40.61705311178274\n",
      "Episode 250, Total Reward: -21.042230740588504\n",
      "Episode 251, Total Reward: 15.225502226953996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):  \u001b[38;5;66;03m# Maximum steps per episode\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     action \u001b[38;5;241m=\u001b[39m select_action(state, epsilon)\n\u001b[1;32m--> 102\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    104\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\DIMITRIS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\DIMITRIS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIMITRIS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIMITRIS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:675\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    672\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\DIMITRIS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:787\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    786\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the NN for the Q-learning agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(DQN,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(128,output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x) # outputs Q-values\n",
    "    \n",
    "    \n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "    \n",
    "# Parameters\n",
    "input_dim = env.observation_space.shape[0]  # 8 state inputs\n",
    "output_dim = env.action_space.n  # 4 discrete actions\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "memory_size = 100000\n",
    "episodes = 1000\n",
    "target_update = 10    \n",
    "     \n",
    "     \n",
    "\n",
    "# Initialize Q-network and target network\n",
    "policy_net = DQN(input_dim, output_dim)\n",
    "target_net = DQN(input_dim, output_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to select an action based on epsilon-greedy policy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()  # Random action (exploration)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = policy_net(state)\n",
    "            return q_values.argmax().item()  # Best action (exploitation)\n",
    "        \n",
    "        \n",
    "# Store experiences in replay memory\n",
    "def store_experience(memory, experience):\n",
    "    memory.append(experience)\n",
    "\n",
    "# Sample a random batch from replay memory\n",
    "def sample_batch(memory, batch_size):\n",
    "    return random.sample(memory, batch_size)\n",
    "\n",
    "# Update the policy network\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = sample_batch(memory, batch_size)\n",
    "    state_batch = torch.FloatTensor([exp[0] for exp in batch])\n",
    "    action_batch = torch.LongTensor([exp[1] for exp in batch]).unsqueeze(1)\n",
    "    reward_batch = torch.FloatTensor([exp[2] for exp in batch])\n",
    "    next_state_batch = torch.FloatTensor([exp[3] for exp in batch])\n",
    "    done_batch = torch.FloatTensor([exp[4] for exp in batch])\n",
    "\n",
    "    # Compute Q(s, a)\n",
    "    q_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute Q(s', a') from the target network (max Q-value for next state)\n",
    "    next_q_values = target_net(next_state_batch).max(1)[0]\n",
    "    next_q_values = next_q_values * (1 - done_batch)  # Zero out if the episode is done\n",
    "    expected_q_values = reward_batch + (gamma * next_q_values)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(q_values.squeeze(), expected_q_values.detach())\n",
    "\n",
    "    # Optimize the network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(1000):  # Maximum steps per episode\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store the experience in replay memory\n",
    "        store_experience(memory, (state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Optimize the model\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon (exploration)\n",
    "    epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "\n",
    "    # Update the target network\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
