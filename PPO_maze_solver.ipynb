{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import logging\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the logger\n",
    "log_filename = \"maze_agent_run.log\"  \n",
    "\n",
    "#delete old log file\n",
    "try:\n",
    "    if os.path.exists(log_filename):\n",
    "        os.remove(log_filename)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set the logging level for urllib3 to WARNING\n",
    "urllib3_logger = logging.getLogger(\"urllib3\")\n",
    "urllib3_logger.setLevel(logging.INFO)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",  # Format for log messages\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),  # Log to a file\n",
    "        #logging.StreamHandler()  # Log to the console\n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "def extract_data(response):\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error code at response\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Retrieve JSON data from response\n",
    "    data = response.json()\n",
    "\n",
    "    # Exctract data\n",
    "    done = data.get('done', None)\n",
    "    info = data.get('info', None)\n",
    "    observation = data.get('observation', None)\n",
    "    reward = data.get('reward', None)\n",
    "    trunc = data.get('truncated', None)\n",
    "\n",
    "    # Convert observation to numpy array if it's not None\n",
    "    if observation is not None:\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "    else:\n",
    "        observation = None\n",
    "\n",
    "    return done, info, observation, reward, trunc\n",
    "    \n",
    "# implement retry policy\n",
    "@retry(stop=stop_after_attempt(5),wait=wait_exponential(multiplier=1,min=4,max=10))\n",
    "def make_request(url,headers,data=None):\n",
    "    if data:\n",
    "        response = requests.post(url,headers=headers,json=data)\n",
    "    else:\n",
    "        response = requests.post(url,headers=headers)\n",
    "    \n",
    "    #raise http error for bad responses\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # delay for api stability\n",
    "    time.sleep(0.1)\n",
    "    logging.info(f\"Action:{url.split('/')[3]} data {data}, obs: {response.json().get('observation')},reward: {response.json().get('reward')} \")\n",
    "    return response\n",
    "\n",
    "\n",
    "def new_game(base_url,headers):\n",
    "    \n",
    "    api_new_game = base_url+\"/new_game\"\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    # Start new game\n",
    "    response = make_request(url = api_new_game, headers=headers)\n",
    "    uuid = response.json().get('uuid')\n",
    "\n",
    "    return uuid\n",
    "\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=1):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.episode_steps = 0\n",
    "        self.episode_rewards = 0\n",
    "        self.episode_lengths = []\n",
    "        self.episode_rewards_list = []\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        # Increment step and reward counters for the current episode\n",
    "        self.episode_steps += 1\n",
    "        self.episode_rewards += self.locals['rewards']\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if self.locals['dones'][0]:\n",
    "            # Log the number of steps and reward for the episode\n",
    "            self.episode_lengths.append(self.episode_steps)\n",
    "            self.episode_rewards_list.append(self.episode_rewards)\n",
    "\n",
    "            logging.info(f\"Episode finished - Steps: {self.episode_steps}, Reward: {self.episode_rewards}\")\n",
    "\n",
    "            # Reset for the next episode\n",
    "            self.episode_steps = 0\n",
    "            self.episode_rewards = 0\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        total_episodes = len(self.episode_lengths)\n",
    "        if total_episodes > 0:\n",
    "            avg_episode_length = sum(self.episode_lengths) / total_episodes\n",
    "            avg_reward_per_episode = sum(self.episode_rewards_list) / total_episodes\n",
    "            logging.info(f\"End of rollout. Total episodes: {total_episodes}, \"\n",
    "                         f\"Average episode length: {avg_episode_length}, \"\n",
    "                         f\"Average reward per episode: {avg_reward_per_episode}\")\n",
    "\n",
    "\n",
    "class MazeAPIEnv(gym.Env):\n",
    "    def __init__(self, api_step_url, headers,api_reset):\n",
    "        super(MazeAPIEnv, self).__init__()\n",
    "        self.headers = headers \n",
    "        self.api_step_url = api_step_url  \n",
    "        self.headers = headers  \n",
    "        self.api_reset = api_reset\n",
    "\n",
    "        # Define the action and observation space\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Defint observation Space 10*10 box\n",
    "        self.observation_space = spaces.Box(low=0, high=4, shape=(2,), dtype=np.float32)  \n",
    "\n",
    "\n",
    "        self.current_state = np.array([0.0, 0.0], dtype=np.float32)  \n",
    "        self.done = False\n",
    "        \n",
    "        #for reward normalization\n",
    "        self.min_reward = -1\n",
    "        self.max_reward = -0.01\n",
    "        \n",
    "    def normalize(self, observation):\n",
    "        \"\"\" Normalize the observation to the range [0, 1]. \"\"\"\n",
    "        return (observation ) / 10 \n",
    "    \n",
    "    def normalize_reward(self,reward):\n",
    "        \"\"\" Normalize reward to rande [-1,1] \"\"\"\n",
    "        return (2*(reward-self.min_reward)/(self.max_reward-self.min_reward))-1\n",
    "            \n",
    "    def reset(self,seed=None,**kwargs):\n",
    "        response = make_request(url=self.api_reset, headers=self.headers)\n",
    "\n",
    "        self.current_state = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        self.done = False\n",
    "        self.visited = set()\n",
    "        \n",
    "        return self.current_state, {}\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        content = {'action': int(action)}\n",
    "        response = make_request(url=self.api_step_url, headers=self.headers, data=content)\n",
    "    \n",
    "        if response.status_code !=200 :\n",
    "            print(\"error code in step\")\n",
    "        \n",
    "        # Extract the response data\n",
    "        done,_,raw_observation,reward,truncated = extract_data(response)\n",
    "        reward = reward+1\n",
    "        \n",
    "        # normalize observation and reward\n",
    "        self.current_state = self.normalize(np.array(raw_observation, dtype=np.float32))\n",
    "        reward = self.normalize_reward(reward)\n",
    "        self.done = done\n",
    "                  \n",
    "        return self.current_state, reward, done,truncated, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### PPO MODEL ########################\n",
    "# Define the API endpoint and headers\n",
    "\n",
    "# Import base url from env.json file  \n",
    "data = open('env.json')    \n",
    "base_url = json.load(data)['url']\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "uuid = new_game(base_url,headers)\n",
    "\n",
    "api_reset = base_url+\"/reset/\"+uuid\n",
    "api_step_url = base_url+\"/step/\"+uuid\n",
    "\n",
    "# Instantiate the custom environment\n",
    "env = MazeAPIEnv(api_step_url, headers,api_reset)\n",
    "\n",
    "\n",
    "# Create the PPO model\n",
    "model = PPO('MlpPolicy', env, \n",
    "            verbose=2,\n",
    "            tensorboard_log=\"C:\\\\Users\\\\DIMITRIS\\\\Reinforcement_Learning\\\\.ppo_tensorboard\",\n",
    "            learning_rate=1e-3,\n",
    "            n_steps=1024,\n",
    "            ent_coef = 0.03\n",
    "            )\n",
    "# Train the model\n",
    "\n",
    "callback = TrainingCallback()\n",
    "\n",
    "model.learn(total_timesteps=5000,progress_bar=True,callback=callback)  # Adjust the number of timesteps as needed\n",
    "\n",
    "\n",
    "model.save('ppo_custom_env-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Import base url from env.json file  \n",
    "    data = open('env.json')    \n",
    "    base_url = json.load(data)['url']\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    uuid = new_game(base_url,headers)\n",
    "\n",
    "\n",
    "    api_reset = base_url+\"/reset/\"+uuid\n",
    "    api_step_url = base_url+\"/step/\"+uuid\n",
    "\n",
    "    # Instantiate the custom environment\n",
    "    env = MazeAPIEnv(api_step_url, headers,api_reset)\n",
    "\n",
    "   \n",
    "    # Replace with your environment and model paths\n",
    "    #model = PPO.load('ppo_custom_env.zip')  # Load your trained model\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=False)\n",
    "    print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
